import os
import subprocess
import zipfile

from fastapi import HTTPException, APIRouter
from fastapi.responses import FileResponse

router = APIRouter(prefix='/endpoints',
                   tags=['trainer'],
                   responses={200: {'description': 'OK'},
                              404: {'description': 'Resource Not Found'},
                              415: {'description': 'Unsupported Media Type'}})


@router.post('/trainer')
async def trainer(model_name_or_path: str, dataset: str, attack: str, task_type: str = 'classification',
                  model_max_length: str = None, model_num_labels: int = None,
                  dataset_train_split: float = None,
                  dataset_eval_split: float = None,
                  filter_train_by_labels: str = None,
                  filter_eval_by_labels: str = None, num_epochs: int = 3,
                  num_clean_epochs: int = 1, attack_epoch_interval: int = 1,
                  early_stopping_epochs: int = None, learning_rate: float = 5e-5,
                  num_warmup_steps: int = 500, weight_decay: float = 0.01,
                  per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 32,
                  gradient_accumulation_steps: int = 1, random_seed: int = 786,
                  parallel: bool = False, load_best_model_at_end: bool = False,
                  alpha: float = 1.0, num_train_adv_examples: int = -1,
                  query_budget_train: float = None,
                  attack_num_workers_per_device: int = 1, output_dir: str = './output',
                  checkpoint_interval_steps: float = None, checkpoint_interval_epochs: int = None,
                  save_last: bool = True, log_to_tb: bool = False,
                  tb_log_dir: str = None, log_to_wandb: bool = False,
                  wandb_project: str = 'textattack', logging_interval_step: int = 1):
    """
    This function is used to call the textattack CLI to run model training using the target system
    
    
    **model_name_or_path**: Name of the model to use or path to the model on the system

    **dataset**: Name of the dataset to use or the Dataset object generated by the user

    **attack**: Attack string

    **task_type**: Action to take while training

    **model_max_length**: Model Max Length

    **model_num_labels**: Number of Labels

    **dataset_train_split**: Train split for dataset

    **dataset_eval_split**: Evaluation split for dataset

    **filter_train_by_labels**: Filter Train Data By Labels

    **filter_eval_by_labels**: Filter Evaluation Data By Labels

    **num_epochs**: Total number of epochs for training

    **num_clean_epochs**: Number of epochs to train on just the original training dataset before adversarial training

    **attack_epoch_interval**: Generate a new adversarial training set every N epochs

    **early_stopping_epochs**: Number of epochs validation must increase before stopping early

    **learning_rate**: Learning rate of the model

    **num_warmup_steps**: The number of steps for the warmup phase of linear scheduler

    **weight_decay**: Weight decay (L2 penalty)

    **per_device_train_batch_size**: The batch size per GPU/CPU for training

    **per_device_eval_batch_size**: The batch size per GPU/CPU for evaluation

    **gradient_accumulation_steps**: Number of updates steps to accumulate the gradients before performing a
                                     backward/update pass

    **random_seed**: Random seed for reproducibility

    **parallel**: Use Multiple GPUs using torch.DataParallel class

    **load_best_model_at_end**: keep track of the best model across training and load it at the end

    **alpha**: The weight for adversarial loss

    **num_train_adv_examples**: The number of samples to successfully attack when generating adversarial training
                                set before start of every epoch

    **query_budget_train**: The max query budget to use when generating adversarial training set

    **attack_num_workers_per_device**: Number of worker processes to run per device for attack

    **output_dir**: Directory to output training logs and checkpoints

    **checkpoint_interval_steps**: Save after N updates

    **checkpoint_interval_epochs**: Save after N epochs

    **save_last**: Save the model at end of training

    **log_to_tb**: Log to Tensorboard

    **tb_log_dir**: Directory to output training logs and checkpoints

    **log_to_wandb**: Log to Wandb

    **wandb_project**: Name of Wandb project for logging

    **logging_interval_step**: Log to Tensorboard/Wandb every N training steps
    """

    var_list = ['textattack', 'train']
    maps = {
        'model_name_or_path': ['--model-name-or-path', model_name_or_path],
        'dataset': ['--dataset', dataset],
        'attack': ['--attack', attack],
        'task_type': ['--task-type', task_type],
        'model_max_length': ['--model-max-length', model_max_length],
        'model_num_labels': ['--model-num-labels', model_num_labels],
        'dataset_train_split': ['--dataset-train-split', dataset_train_split],
        'dataset_eval_split': ['--dataset-eval-split', dataset_eval_split],
        'filter_train_by_labels': ['--filter-train-by-labels', filter_train_by_labels],
        'filter_eval_by_labels': ['--filter-eval-by-labels', filter_eval_by_labels],
        'num_epochs': ['--num-epochs', num_epochs],
        'num_clean_epochs': ['--num-clean-epochs', num_clean_epochs],
        'attack_epoch_interval': ['--attack-epoch-interval', attack_epoch_interval],
        'early_stopping_epochs': ['--early-stopping-epochs', early_stopping_epochs],
        'learning_rate': ['--learning-rate', learning_rate],
        'num_warmup_steps': ['--num-warmup-steps', num_warmup_steps],
        'weight_decay': ['--weight-decay', weight_decay],
        'per_device_train_batch_size': ['--per-device-train-batch-size', per_device_train_batch_size],
        'per_device_eval_batch_size': ['--per-device-eval-batch-size', per_device_eval_batch_size],
        'gradient_accumulation_steps': ['--gradient-accumulation-steps', gradient_accumulation_steps],
        'random_seed': ['--random-seed', random_seed],
        'parallel': ['--parallel', parallel],
        'load_best_model_at_end': ['--load-best-model-at-end', load_best_model_at_end],
        'alpha': ['--alpha', alpha],
        'num_train_adv_examples': ['--num-train-adv-examples', num_train_adv_examples],
        'query_budget_train': ['--query-budget-train', query_budget_train],
        'attack_num_workers_per_device': ['--attack-num-workers-per-device', attack_num_workers_per_device],
        'output_dir': ['--output-dir', output_dir],
        'checkpoint_interval_steps': ['--checkpoint-interval-steps', checkpoint_interval_steps],
        'checkpoint_interval_epochs': ['--checkpoint-interval-epochs', checkpoint_interval_epochs],
        'save_last': ['--save-last', save_last],
        'log_to_tb': ['--log-to-tb', log_to_tb],
        'tb_log_dir': ['--tb-log-dir', tb_log_dir],
        'log_to_wandb': ['--log-to-wandb', log_to_wandb],
        'wandb_project': ['--wandb-project', wandb_project],
        'logging_interval_step': ['--logging-interval-step', logging_interval_step]
    }
    maps = {key: value for key, value in maps.items() if value[1] is not None}
    for k, v in maps.items():
        var_list.extend(v)

    var_list = [str(iter_) for iter_ in var_list if type(iter_) is not bool]

    try:
        subprocess.run(var_list)
    except Exception as ex:
        raise HTTPException(status_code=415, detail=ex)
    else:
        if output_dir is not None:
            try:
                with zipfile.ZipFile('file.zip', 'w') as zipped:
                    for folder, subfolder, fnames in os.walk(output_dir):
                        for fname in fnames:
                            fpath = os.path.join(folder, fname)
                            zipped.write(fpath, os.path.basename(fpath))
            except Exception as ex:
                raise HTTPException(status_code=404, detail=ex)
            else:
                return FileResponse('file.zip', media_type='application/zip', filename='file.zip')
        else:
            raise HTTPException(status_code=404, detail='Error: The model directory is not found. Try again.')
